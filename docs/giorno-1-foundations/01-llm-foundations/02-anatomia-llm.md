---
title: Anatomia di un LLM
description: Comprendi come funziona un LLM sotto il cofano, senza tecnicismi eccessivi
sidebar_position: 2
tags: [llm-foundations, architettura, training, inference]
---

# Come Funziona un LLM

> ‚è±Ô∏è **Durata**: 15 minuti
> üéØ **Livello**: Base

---

## üéØ Learning Objectives

Al termine di questa sezione sarai in grado di:

- [ ] Comprendere l'architettura base di un LLM (transformer) senza matematica
- [ ] Distinguere chiaramente Training da Inference
- [ ] Confrontare modelli (GPT, Claude, Gemini) e scegliere per il tuo use case

---

## üìö Architettura LLM Semplificata

:::tip üí° Analogia: LLM come Super-Calcolatrice Linguistica
Un LLM √® essenzialmente una **calcolatrice di probabilit√†** incredibilmente sofisticata che:
- Prende testo in input
- Calcola quale parola/token viene probabilmente dopo
- Ripete il processo per generare risposte complete
:::

---

### Componenti Base (No Matematica!)

**1. Input Layer - Trasformazione Testo ‚Üí Numeri**

```
Testo: "Il gatto √®"
   ‚Üì
Tokenizzazione
   ‚Üì
Token: ["Il", " gatto", " √®"]
   ‚Üì
Embeddings (numeri che rappresentano significato)
   ‚Üì
[0.23, -0.45, 0.89, ...] (vettori)
```

Gli LLM non "capiscono" il testo come noi. Tutto diventa numeri (vettori) che rappresentano significati.

---

**2. Transformer Layers - Elaborazione**

Il cuore dell'LLM: decine/centinaia di "strati" neurali che:
- Analizzano relazioni tra parole
- Identificano pattern linguistici
- Calcolano contesto e significato

:::info üî¨ Quanto √® Grande?
- **GPT-5**: Sistema unificato multi-modello (dimensione non dichiarata)
- **Claude Sonnet 4**: Non dichiarato ufficialmente (stimato ~500+ miliardi)
- **Gemini 2.5 Pro**: Non dichiarato (stimato ~1+ trilione)

**Parametri** = i "pesi" che determinano come il modello elabora informazioni. I modelli 2025 usano architetture pi√π efficienti, quindi dimensione non √® tutto.
:::

**Analogia**: Pensa ai parametri come alle connessioni sinaptiche nel cervello umano. Pi√π connessioni = pi√π "conoscenza" potenziale.

---

**3. Output Layer - Probabilit√† Next Token**

```
Input: "Il gatto √® seduto sul..."

LLM calcola probabilit√†:
- "tappeto" ‚Üí 45%
- "divano" ‚Üí 30%
- "tavolo" ‚Üí 15%
- "tetto" ‚Üí 8%
- altro ‚Üí 2%

Scelta (influenzata da temperature):
‚Üí "tappeto"
```

L'LLM **non sta pensando**. Sta calcolando statisticamente quale token √® pi√π probabile basandosi su:
- Pattern visti durante training
- Contesto fornito
- Parametri del modello

---

## üèãÔ∏è Training vs Inference

### Training: Creare il Modello

**Cos'√®**: La fase di "apprendimento" (mesi, datacenter enormi)

**Processo**:
1. **Raccolta Dati**: Miliardi di testi da internet, libri, codice
2. **Training**: Il modello "legge" tutto e impara pattern linguistici
3. **Aggiustamento Pesi**: Parametri vengono ottimizzati per predire correttamente
4. **Fine-Tuning**: Raffinamento su compiti specifici
5. **Alignment (RLHF)**: Addestramento per sicurezza e utilit√†

**Chi lo fa**: OpenAI, Anthropic, Google (non gli utenti!)

**Costi**: Milioni di dollari, mesi di tempo, datacenter enormi

:::warning üí° Importante
Come utente, **NON fai training**. Usi modelli gi√† addestrati.

Training = studiare all'universit√† per 5 anni
:::

---

### Inference: Usare il Modello

**Cos'√®**: La fase di "utilizzo" (quando usiamo ChatGPT/Claude)

**Processo**:
1. Scrivi prompt
2. LLM applica conoscenza appresa
3. Genera risposta token-by-token
4. Ti mostra risultato

**Chi lo fa**: Tu, ogni volta che usi ChatGPT/Claude

**Costi**: Centesimi per richiesta (o ‚Ç¨20/mese subscription)

:::tip ‚úÖ Questo √à Inference
Ogni volta che:
- Chiedi a ChatGPT qualcosa
- Claude scrive codice per te
- Generi un'immagine con DALL-E

Stai facendo **inference**, non training.
:::

---

### Confronto Training vs Inference

| Aspetto | Training | Inference |
|---------|----------|-----------|
| **Quando** | Una volta (per creare modello) | Ogni volta che lo usi |
| **Chi** | Aziende AI (OpenAI, Anthropic) | Tu (utente finale) |
| **Costi** | Milioni $ | Centesimi |
| **Tempo** | Settimane/Mesi | Secondi |
| **Hardware** | Datacenter, 1000+ GPU | 1 computer/server |
| **Cosa fa** | Impara pattern dai dati | Applica conoscenza appresa |

---

## ü§ñ Confronto tra Modelli

### Tabella Comparativa

| Caratteristica | GPT-5 (OpenAI) | Claude Sonnet 4 (Anthropic) | Gemini 2.5 Pro (Google) |
|---|---|---|---|
| **Context Window** | 400K token<br/>(~1,200 pagine) | 1M token<br/>(~3,000 pagine) | 1M token<br/>(~3,000 pagine) |
| **Punti di Forza** | ‚Ä¢ Math/Coding eccezionale (94.6% AIME)<br/>‚Ä¢ Ecosystem maturo (API, plugin)<br/>‚Ä¢ Varianti economiche (mini, nano) | ‚Ä¢ Context pi√π grande sul mercato<br/>‚Ä¢ Precisione e affidabilit√† top<br/>‚Ä¢ Prompt caching per long context | ‚Ä¢ Multimodalit√† nativa (video, audio)<br/>‚Ä¢ Google Workspace integrato<br/>‚Ä¢ Web search nativo sempre attivo |
| **Punti di Debolezza** | ‚Ä¢ Context limitato vs altri<br/>‚Ä¢ Pricing pi√π alto | ‚Ä¢ Pricing alto oltre 200K context<br/>‚Ä¢ A volte troppo cauto | ‚Ä¢ Meno maturo vs GPT/Claude<br/>‚Ä¢ Adoption enterprise limitata |
| **Ideale Per** | Coding, math, reasoning quantitativo, API consolidate | Documenti enormi, precisione assoluta, compliance/legal | Google Workspace, multimodalit√†, web search real-time |
| **Pricing (indicativo)** | $$$ (flagship)<br/>$ (mini/nano economici) | $$ (sotto 200K)<br/>$$$ (oltre 200K) | $$ (Pro)<br/>$ (Flash/Flash-Lite) |

:::tip üí° Quick Decision Guide
- **Documenti enormi (100+ pagine)?** ‚Üí Claude Sonnet 4 o Gemini 2.5 Pro
- **Coding/Math complesso?** ‚Üí GPT-5
- **Lavori in Google Workspace?** ‚Üí Gemini 2.5 Pro
- **Budget limitato ma qualit√†?** ‚Üí GPT-5 mini/nano o Gemini Flash
:::

---

### Varianti dei Modelli

**GPT-5 Famiglia**:
- **GPT-5**: Flagship, deep reasoning
- **GPT-5 mini**: Balance qualit√†/velocit√†/costo
- **GPT-5 nano**: Task semplici, alta volume, budget ridotto

**Claude Famiglia**:
- **Claude Sonnet 4**: Modello principale unificato 2025

**Gemini Famiglia**:
- **Gemini 2.5 Pro**: Deep Think, reasoning complesso
- **Gemini 2.5 Flash**: Velocit√† ottimizzata
- **Gemini 2.5 Flash-Lite**: Costi minimizzati

---

## üí° Come Scegliere il Modello

### Decision Tree

```
Hai documenti enormi da analizzare? (>100 pagine, codebase completi)
‚îú‚îÄ S√å ‚Üí Claude Sonnet 4 (1M context) o Gemini 2.5 Pro
‚îî‚îÄ NO ‚Üí ‚Üì

Fai principalmente coding/math complesso?
‚îú‚îÄ S√å ‚Üí GPT-5 (94.6% AIME, 74.9% SWE-bench)
‚îî‚îÄ NO ‚Üí ‚Üì

Usi Google Workspace come hub centrale?
‚îú‚îÄ S√å ‚Üí Gemini 2.5 Pro (integrazione nativa Gmail, Drive, Meet)
‚îî‚îÄ NO ‚Üí ‚Üì

Budget limitato ma serve qualit√†?
‚îú‚îÄ S√å ‚Üí GPT-5 mini/nano (ottimo rapporto qualit√†/prezzo)
‚îî‚îÄ NO ‚Üí ‚Üì

Precisione assoluta critica? (legal, medical, compliance)
‚îú‚îÄ S√å ‚Üí Claude Sonnet 4
‚îî‚îÄ NO ‚Üí GPT-5 o Gemini 2.5 Pro (generale)
```

---

## üî¨ Parametri Modello: Pi√π Grande = Migliore?

**Risposta breve**: No, non sempre.

### Dimensione vs Capability

| Modello | Architettura | Use Case Ottimale |
|---------|--------------|-------------------|
| GPT-5 | Sistema unificato multi-model | Task complessi, reasoning, coding |
| GPT-5 mini | Modello ottimizzato | Balance qualit√†/velocit√†/costo |
| GPT-5 nano | Modello lightweight | Task semplici, alte volume, budget |
| Claude Sonnet 4 | Modello unificato | Documenti massivi, precisione |
| Gemini 2.5 Pro | Deep Think enabled | Reasoning complesso, multimodale |
| Gemini 2.5 Flash | Ottimizzato velocit√† | Processing alto volume |

**Trade-off 2025**:
- Modelli flagship: pi√π capability, context enorme, costi pi√π alti
- Modelli ottimizzati (mini, Flash): velocit√†, costi bassi, qualit√† ancora buona
- Architetture moderne: efficienza migliorata (dimensione ‚â† tutto)

:::tip üí° Regola Pratica
Usa il **modello giusto** per il task (non sempre il pi√π grande).

- Task routine (emails, summaries) ‚Üí Modelli economici (GPT-5 nano, Gemini Flash-Lite)
- Task complessi (analysis, coding) ‚Üí Flagship (GPT-5, Claude Sonnet 4, Gemini 2.5 Pro)
- Documenti enormi (>500 pagine) ‚Üí Long context (Claude Sonnet 4: 1M tokens)
- Budget ottimizzato ‚Üí GPT-5 mini (ottimo rapporto qualit√†/prezzo)
:::

---

## ‚ö†Ô∏è Common Misconceptions

### ‚ùå Mito 1: "LLM Capisce come un Umano"

**Realt√†**: LLM fa **pattern matching statistico**. Non ha:
- Vera comprensione
- Coscienza
- Modello del mondo reale
- Intenzionalit√†

**Implicazione**: Pu√≤ fare errori logici che un bambino non farebbe.

---

### ‚ùå Mito 2: "Posso Addestrare GPT sui Miei Dati"

**Realt√†**: Come utente, **non puoi fare training** (troppo costoso).

Puoi fare:
- ‚úÖ **Prompt Engineering**: ottimizzare input
- ‚úÖ **Fine-Tuning** (OpenAI API): leggero re-training ($$$)
- ‚úÖ **RAG** (Retrieval Augmented Generation): fornire documenti nel contesto

Per 99% use case: **Prompt Engineering √® sufficiente**.

---

### ‚ùå Mito 3: "LLM Ha Accesso a Internet"

**Realt√†**: Dipende dal modello e configurazione:
- GPT-5 (ChatGPT): S√å, con web browsing attivato
- Claude Sonnet 4: S√å, se abiliti web search (beta)
- Gemini 2.5 Pro: S√å, web search nativo sempre attivo
- API GPT-5/Claude (base): NO (solo training data)

**Knowledge Cutoff**: Modelli hanno data limite training:
- GPT-5: settembre 2025
- Claude Sonnet 4: agosto 2025
- Gemini 2.5 Pro: marzo 2025 (ma web search compensa)

Post-cutoff: usa web search o fornisci documenti aggiornati.

---

## üîë Key Takeaways

:::tip ‚ú® Punti Chiave da Ricordare

**Architettura**:
- LLM = calcolatrice probabilit√† linguistica (transformer)
- Input testo ‚Üí numeri ‚Üí elaborazione ‚Üí output probabilit√† token

**Training vs Inference**:
- **Training**: creare modello (aziende AI, milioni $, mesi)
- **Inference**: usare modello (tu, centesimi, secondi)
- Come utente: fai solo inference, non training

**Scegliere Modello**:
- **Claude Sonnet 4**: documenti enormi (1M context), precisione massima, compliance
- **GPT-5**: coding/math avanzati, reasoning, integrazioni API mature
- **Gemini 2.5 Pro**: Google Workspace, multimodalit√† nativa, web search integrato
- **Budget**: GPT-5 mini/nano, Gemini Flash/Flash-Lite

**Parametri**:
- Pi√π grande ‚â† sempre migliore
- Usa modello pi√π piccolo che risolve il problema
- Trade-off: capability vs velocit√† vs costo

**Misconceptions**:
- LLM non "capisce" veramente
- Non puoi fare training (troppo costoso)
- Knowledge cutoff esiste ‚Üí usa web search per info recenti

:::

---

## üîó Risorse Aggiuntive

**Approfondimenti**:
- üìñ [Transformer Architecture Explained](https://jalammar.github.io/illustrated-transformer/) - Visualizzazione interattiva
- üìñ [GPT vs Claude Comparison](https://www.anthropic.com/claude) - Docs ufficiali
- üìñ [Glossario - Parametri](/risorse/glossario#parametri) - Definizione tecnica

**Tool**:
- üîß [Artificial Analysis](https://artificialanalysis.ai) - Confronto performance modelli
- üîß [LLM Benchmarks](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) - Ranking community

---

## ‚û°Ô∏è Prossimi Passi

Ora che comprendi come funzionano gli LLM:

1. **Rifletti**: Quale modello usi di solito? √à il pi√π adatto ai tuoi task?
2. **Sperimenta**: Prova lo stesso prompt su GPT-4 vs Claude, confronta risultati
3. **Continua**: [Come "Ragionano" gli LLM](ragionamento) - Next token prediction

---

:::info üí¨ Discussione
**Domande da considerare**:
- Hai mai notato differenze di qualit√† tra GPT e Claude?
- Quando ti serve context window grande vs quando basta uno piccolo?
- Per i tuoi use case, quale modello sarebbe ottimale?
:::
